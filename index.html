<!DOCTYPE html>

<title>Deceptive-NeRF</title>

<!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-21408087-2"></script>
<script>
    window.dataLayer = window.dataLayer || [];

    function gtag() {
        dataLayer.push(arguments);
    }
    gtag('js', new Date());

    gtag('config', 'UA-21408087-2');
</script>

<meta charset="utf-8">

<link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/4.4.0/css/bootstrap.min.css" crossorigin="anonymous">
<link rel="stylesheet" href="css/style.css">

<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

<body>
    <div class="container">
        <div class="row mb-2 mt-4" id="paper-title">
            <h2 class="col-md-12 text-center">
                Deceptive-NeRF: Enhancing NeRF Reconstruction
            </h2>
            <h2 class="col-md-12 text-center">
                using Pseudo-Observations from Diffusion Models
            </h2>
        </div>

        <div class="row" id="authors" >
            <div class="mx-auto text-center" >
                <ul class="list-inline mb-0">
                    <li class="list-inline-item">
                        <a target="_blank" href="http://xinhangliu.com/" style="font-size: 13pt;">Xinhang Liu</a><br/>
                        <span style="font-size: 11pt;">HKUST</span>
                    </li>

                    <li class="list-inline-item">
                        <a target="_blank" href="https://danielshkao.github.io/" style="font-size: 13pt;"> Shiu-hong Kao </a><br/>
                        <span style="font-size: 11pt;">HKUST</span>
                    </li>

                    <li class="list-inline-item">
                        <a target="_blank" href="https://jiabenchen.github.io/" style="font-size: 13pt;">Jiaben Chen</a><br/>
                        <span style="font-size: 11pt;">UC San Diego</span>
                    </li>


                    <li class="list-inline-item">
                        <a target="_blank" href="https://scholar.google.com/citations?user=nFhLmFkAAAAJ&hl" style="font-size: 13pt;">Yu-Wing Tai</a><br/>
                        <span style="font-size: 11pt;">HKUST</span>
                    </li>

                    <li class="list-inline-item">
                        <a target="_blank" href="https://scholar.google.com/citations?user=EWfpM74AAAAJ&hl" style="font-size: 13pt;">Chi-Keung Tang</a> <br/>
                        <span style="font-size: 11pt;">HKUST</span>
                    </li>
                </ul>

                <!--</ul>
                <ul class="list-inline mb-0" id="institution">
                    <li class="list-inline-item">
                        <sup>1</sup>
                        HKUST
                    </li>
                    <li class="list-inline-item">
                        <sup>2</sup>
                        UC San Diego
                    </li>
                </ul>-->
            </div>
        </div>
        
        <div class="row mb-2" id="links">
            <div class="mx-auto">
                <ul class="nav">
                    <li class="nav-item text-center">
                        <a href="https://arxiv.org/pdf/2305.15171.pdf" class="nav-link" title="Temp link">
                            <svg style="width:48px;height:48px" viewBox="0 0 24 24">
                                <path fill="currentColor" d="M16 0H8C6.9 0 6 .9 6 2V18C6 19.1 6.9 20 8 20H20C21.1 20 22 19.1 22 18V6L16 0M20 18H8V2H15V7H20V18M4 4V22H20V24H4C2.9 24 2 23.1 2 22V4H4M10 10V12H18V10H10M10 14V16H15V14H10Z" />
                            </svg><br>
                            Paper
                        </a>
                    </li>
                    <!--<li class="nav-item text-center">
                        
                        <a href="" class="nav-link">
                            <svg style="width:48px;height:48px" viewBox="0 0 65 65">
                                <path fill="currentColor" d="M32 0a32.021 32.021 0 0 0-10.1 62.4c1.6.3 2.2-.7 2.2-1.5v-6c-8.9 1.9-10.8-3.8-10.8-3.8-1.5-3.7-3.6-4.7-3.6-4.7-2.9-2 .2-1.9.2-1.9 3.2.2 4.9 3.3 4.9 3.3 2.9 4.9 7.5 3.5 9.3 2.7a6.93 6.93 0 0 1 2-4.3c-7.1-.8-14.6-3.6-14.6-15.8a12.27 12.27 0 0 1 3.3-8.6 11.965 11.965 0 0 1 .3-8.5s2.7-.9 8.8 3.3a30.873 30.873 0 0 1 8-1.1 30.292 30.292 0 0 1 8 1.1c6.1-4.1 8.8-3.3 8.8-3.3a11.965 11.965 0 0 1 .3 8.5 12.1 12.1 0 0 1 3.3 8.6c0 12.3-7.5 15-14.6 15.8a7.746 7.746 0 0 1 2.2 5.9v8.8c0 .9.6 1.8 2.2 1.5A32.021 32.021 0 0 0 32 0z" />
                            </svg>
                            <br>
                            Code
                        </a>
                    </li>-->
                </ul>
            </div>
        </div>
        
        <div class="row mb-3 pt-2">
            <div class="col-md-8 mx-auto">
                <b>Comment:</b> Code is coming soon!
            </div>
        </div>
        <!-- <div class="row mb-3 pt-2">
            <div class="col-md-8 mx-auto">
                <div class="embed-responsive embed-responsive-16by9 pb-3">
                    <iframe class="embed-responsive-item" src="https://www.youtube.com/embed/KCDd7UFO1d0" frameborder="0" allow="accelerometer; autoplay muted; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
                </div>
                <p class="text-justify mt-2 pt-3">
                    We propose a view-dependent sparse voxel model, Plenoxel <em>(plenoptic volume element)</em>, that can optimize to the same fidelity as <a href="http://tancik.com/nerf">Neural Radiance Fields (NeRFs)</a>
                    without any neural networks. Our typical optimization time is 11 minutes on a single GPU, a speedup of two orders of magnitude compared to NeRF.
                </p>
            </div>
        </div> -->
        <div class="row mb-3 pt-2">
            <div class="col-md-8 mx-auto">
                <center><video width="90%" controls>
                    <source src="src/video_new.mp4" type="video/mp4">
                </video></center>
                
                <h4>Abstract</h4>
                <p class="text-justify">
                    This paper introduces Deceptive-NeRF, a new method for enhancing the quality of reconstructed NeRF models using synthetically generated pseudo-observations, capable of handling sparse input and removing floater artifacts. Our proposed method involves three key steps: 1) reconstruct a coarse NeRF model from sparse inputs; 2) generate pseudo-observations based on the coarse model; 3) refine the NeRF model using pseudo-observations to produce a high-quality reconstruction. To generate photo-realistic pseudo-observations that faithfully preserve the identity of the reconstructed scene while remaining consistent with the sparse inputs, we develop a rectification latent diffusion model that generates images conditional on a coarse RGB image and depth map, which are derived from the coarse NeRF and latent text embedding from input images. Extensive experiments show that our method is effective and can generate perceptually high-quality NeRF even with very sparse inputs.
                </p>
                <h4>Overview</h4>
                <img class="img-method" src="src/method.png" style="max-width: 100%;">
                <p class="text-justify">
                    <ol>
                        <li>
                            Given a sparse set of <span style="color: blue;">input images</span> associated with their camera poses, we first train a coarse NeRF, which renders <span style="color: rgb(192,0,0);">coarse novel view</span> images and depth maps. 
                        </li> 
                        <li>
                             We use a rectification latent diffusion model to fine-tune RGB-D images from the coarse NeRF to synthesize <span style="color: rgb(0,128,0);">pseudo-observations</span> from corresponding viewpoints. 
                        </li>
                    <li>
                        We train a fine NeRF using both input images (real) and pseudo-observations (fake) as our final reconstruction of the scene while enforcing consistency across the fake images from different viewpoints.
                    </li>
                    </ol>
                </p>
                <h4>Comparison with few-shot NeRF baselines</h4>
                <img class="img-results" src="src/comparison_new.png" style="max-width: 100%;">
                <p>Corresponding ground truths of novel views are shown on the left. Compared to the baseline FreeNeRF and DietNeRF, our synthesized novel views are more photo-realistic, thanks to the generated pseudo-observations. In contrast, baselines tend to generate blurry results, and even erroneously remove objects near the cameras.</p>
            
            </div>
        </div>
        <div class="row mb-4">
            <div class="col-md-8 mx-auto">
                <h4 class="mb-3">BibTeX</h4>
                <p>If you find this work helpful, please consider citing: </p>
                <textarea id="bibtex" class="form-control" style="height: 110pt; font-size: 11pt;" readonly>
@article{liu2023deceptive,
title={Deceptive-NeRF: Enhancing NeRF Reconstruction using Pseudo-Observations from Diffusion Models},
author={Liu, Xinhang and Kao, Shiu-hong and Chen, Jiaben and Tai, Yu-Wing and Tang, Chi-Keung},
journal={arXiv preprint arXiv:2305.15171},
year={2023}
}</textarea>
            </div>
        </div>
    </div> <!-- container -->
    <script>
        window.mobileAndTabletCheck = function() {
            let check = false;
            (function(a) {
                if (/(android|bb\d+|meego).+mobile|avantgo|bada\/|blackberry|blazer|compal|elaine|fennec|hiptop|iemobile|ip(hone|od)|iris|kindle|lge |maemo|midp|mmp|mobile.+firefox|netfront|opera m(ob|in)i|palm( os)?|phone|p(ixi|re)\/|plucker|pocket|psp|series(4|6)0|symbian|treo|up\.(browser|link)|vodafone|wap|windows ce|xda|xiino|android|ipad|playbook|silk/i.test(a) || /1207|6310|6590|3gso|4thp|50[1-6]i|770s|802s|a wa|abac|ac(er|oo|s\-)|ai(ko|rn)|al(av|ca|co)|amoi|an(ex|ny|yw)|aptu|ar(ch|go)|as(te|us)|attw|au(di|\-m|r |s )|avan|be(ck|ll|nq)|bi(lb|rd)|bl(ac|az)|br(e|v)w|bumb|bw\-(n|u)|c55\/|capi|ccwa|cdm\-|cell|chtm|cldc|cmd\-|co(mp|nd)|craw|da(it|ll|ng)|dbte|dc\-s|devi|dica|dmob|do(c|p)o|ds(12|\-d)|el(49|ai)|em(l2|ul)|er(ic|k0)|esl8|ez([4-7]0|os|wa|ze)|fetc|fly(\-|_)|g1 u|g560|gene|gf\-5|g\-mo|go(\.w|od)|gr(ad|un)|haie|hcit|hd\-(m|p|t)|hei\-|hi(pt|ta)|hp( i|ip)|hs\-c|ht(c(\-| |_|a|g|p|s|t)|tp)|hu(aw|tc)|i\-(20|go|ma)|i230|iac( |\-|\/)|ibro|idea|ig01|ikom|im1k|inno|ipaq|iris|ja(t|v)a|jbro|jemu|jigs|kddi|keji|kgt( |\/)|klon|kpt |kwc\-|kyo(c|k)|le(no|xi)|lg( g|\/(k|l|u)|50|54|\-[a-w])|libw|lynx|m1\-w|m3ga|m50\/|ma(te|ui|xo)|mc(01|21|ca)|m\-cr|me(rc|ri)|mi(o8|oa|ts)|mmef|mo(01|02|bi|de|do|t(\-| |o|v)|zz)|mt(50|p1|v )|mwbp|mywa|n10[0-2]|n20[2-3]|n30(0|2)|n50(0|2|5)|n7(0(0|1)|10)|ne((c|m)\-|on|tf|wf|wg|wt)|nok(6|i)|nzph|o2im|op(ti|wv)|oran|owg1|p800|pan(a|d|t)|pdxg|pg(13|\-([1-8]|c))|phil|pire|pl(ay|uc)|pn\-2|po(ck|rt|se)|prox|psio|pt\-g|qa\-a|qc(07|12|21|32|60|\-[2-7]|i\-)|qtek|r380|r600|raks|rim9|ro(ve|zo)|s55\/|sa(ge|ma|mm|ms|ny|va)|sc(01|h\-|oo|p\-)|sdk\/|se(c(\-|0|1)|47|mc|nd|ri)|sgh\-|shar|sie(\-|m)|sk\-0|sl(45|id)|sm(al|ar|b3|it|t5)|so(ft|ny)|sp(01|h\-|v\-|v )|sy(01|mb)|t2(18|50)|t6(00|10|18)|ta(gt|lk)|tcl\-|tdg\-|tel(i|m)|tim\-|t\-mo|to(pl|sh)|ts(70|m\-|m3|m5)|tx\-9|up(\.b|g1|si)|utst|v400|v750|veri|vi(rg|te)|vk(40|5[0-3]|\-v)|vm40|voda|vulc|vx(52|53|60|61|70|80|81|83|85|98)|w3c(\-| )|webc|whit|wi(g |nc|nw)|wmlb|wonu|x700|yas\-|your|zeto|zte\-/i.test(a.substr(0, 4))) check = true;
            })(navigator.userAgent || navigator.vendor || window.opera);
            return check;
        };

        if (window.location.host.indexOf('alexyu.net') > -1 && window.location.protocol != "https:") {
            // Force HTTPS
            window.location.protocol = "https";
        }


        if (mobileAndTabletCheck()) {
            document.getElementById('demo-warning').style.display = 'block';
            document.getElementById('demo-container').style.display = 'none';
            document.getElementById('demo-warning').innerHTML = "Unfortunately, mobile and tablet devices are not currently supported due to WebGL compatibility issues. We hope to support this in the future.";
        } else {
            var canvas = document.createElement('canvas');
            var gl = canvas.getContext('webgl');
            var tex_limit = gl.getParameter(gl.MAX_TEXTURE_SIZE);
            if (gl && gl instanceof WebGLRenderingContext) {
                const REQUIRED_TEX_LIMIT = 8192;
                if (tex_limit < REQUIRED_TEX_LIMIT) {
                    document.getElementById('demo-warning').style.display = 'block';
                    document.getElementById('demo-container').style.display = 'none';
                    document.getElementById('demo-warning').innerHTML = "Your GPU's maximum texture size is: " + tex_limit + " which is less than the minimum required (" + REQUIRED_TEX_LIMIT + ").  Please try another device, if possible.";
                }
            } else {
                document.getElementById('demo-warning').style.display = 'block';
                document.getElementById('demo-container').style.display = 'none';
                document.getElementById('demo-warning').innerHTML = "Your browser does not support WebGL, or WebGL was disabled. Please use a modern browser like Chrome or Firefox.";
            }
        }
    </script>
</body>